{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Fairness Data Generation and Question Answering System","text":"<p>Transparent tools and standardized benchmarks for fair, explainable, and accountable generative AI.</p> <p>The rapid growth of generative AI brings powerful capabilities\u2014but it also magnifies long-standing concerns around bias, fairness, and representation. Many models reproduce stereotypes embedded in training data, especially around demographic attributes (e.g., gender, ethnicity, age). This project enables systematic, controlled experimentation so researchers and practitioners can pinpoint when and why bias occurs\u2014and what actually mitigates it.</p>"},{"location":"#what-is-the-project-about","title":"\ud83c\udf0d What is the project about?","text":"<p>The AI Fairness Data Generation and Question Answering System is part of Vector Institute's contribution to the broader AIXPERT Project, a multi-institutional initiative, to develop tools and benchmarks for fairness-aware data generation and evaluation in generative AI.</p> <p>It provides:</p> <ul> <li>Controlled synthetic datasets to isolate bias-inducing factors safely and reproducibly.</li> <li>Agentic automation (CrewAI + custom LLM agents) for prompt generation, content creation, metadata, and QC.</li> <li>Fairness metrics &amp; explainers to visualize model behavior and surface disparities.</li> <li>Open, configurable pipelines aligned with responsible AI practices and emerging governance needs.</li> </ul>"},{"location":"#objectives","title":"Objectives","text":"<ul> <li> <p>Develop a Controlled Data Pipeline     Create a reproducible, configurable pipeline for generating text, image, and video with precise control over demographic and contextual variables.</p> </li> <li> <p>Enable Fairness-Aware Benchmarking     Provide tools to build matched baseline vs. fairness-aware datasets for bias diagnosis and mitigation experiments.</p> </li> <li> <p>Support Multi-Domain Risk Analysis     Generate multimodal data for hiring, healthcare, legal, education, and more, covering risks like bias, toxicity, misinformation.</p> </li> <li> <p>Integrate Agentic AI for Automation     Orchestrate generation and QC with CrewAI and custom LLM agents (prompts, assets, annotations, validation).</p> </li> <li> <p>Advance Interpretability &amp; Explainability     Combine zero-shot LLM explainers and fairness metrics to produce interpretable assessments and visualizations.</p> </li> <li> <p>Foster Open Research &amp; Collaboration     Share configs, tools, and docs openly to enable reproducible research and transparent governance.</p> </li> </ul>"},{"location":"#pipeline","title":"Pipeline","text":""},{"location":"#recent-updates","title":"Recent updates","text":"<ul> <li> Released data generation pipeline (multimodal, configurable, agent-orchestrated).</li> <li> Single-agent pipeline prototype for rapid dataset bootstrapping.</li> <li> NeurIPS 2025 LLM-eval Workshop paper: Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment</li> <li> Preprint: TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems</li> <li> TechRxiv article: Responsible Agentic Reasoning and AI Agents\u2014A Critical Survey</li> <li> Poster: Single-Agent TRiSM (NeurIPS LAW)</li> </ul> <p>Have feedback or want to contribute? See the  Team page and open an issue or pull request.</p>"},{"location":"#license","title":"License","text":"<p>This code in this repo is released under the MIT License.</p>"},{"location":"about/","title":"About AIXPERT","text":"<p>The AIXpert Project is a large-scale, multi-institutional research initiative uniting 17 partners under the Horizon Europe Research and Innovation Programme (Grant Agreement No. 101214389) and the Swiss State Secretariat for Education, Research and Innovation (SERI).</p> <p>It aims to transform how AI is developed, deployed, and trusted by society, through a comprehensive framework for explainable, transparent, and accountable artificial intelligence.</p>"},{"location":"about/#vision","title":"Vision","text":"<p>An agentic, multi-layer, GenAI-powered backbone to make AI systems explainable, accountable, and human-centered. The project envisions an adaptable and trustworthy AI ecosystem that integrates explainability, fairness, autonomy, and robustness at every stage of AI development.</p>"},{"location":"about/#objectives","title":"Objectives","text":"<ul> <li> <p>Build an adaptable, explainable AI-agentic platform   Develop interoperable modules that connect explainability, accountability, and fairness.</p> </li> <li> <p>Define and assess AI trustworthiness   Establish measurable criteria and indicators for evaluating the reliability and ethical alignment of AI systems.</p> </li> <li> <p>Advance explainable multimodal foundation models   Drive research in interpretable vision\u2013language\u2013reasoning systems.</p> </li> <li> <p>Demonstrate real-world impact through pilot use cases   Validate the framework across sectors including healthcare, employment, and education.</p> </li> </ul>"},{"location":"about/#consortium-partners","title":"Consortium Partners","text":"<p>The AIXPERT consortium brings together leading academic, research, and industry organizations across Europe:</p> <p>Athena Research Center, University of Barcelona, Amsterdam UMC, KYKLOS, Workable, University of Groningen, Philips, Sorbonne University, CNRS, Furhat Robotics, Orfium, Novelcore, Infinity Design Labs, ITML, Barcelona Supercomputing Center, Martel Innovate, and the Vector Institute (Canada).</p> <p>The Vector Institute contributes expertise in Responsible AI, bias detection, fairness benchmarking, and explainable data generation pipelines within the global AIXPERT ecosystem.</p>"},{"location":"about/#learn-more","title":"Learn More","text":"<ul> <li> Official Project Website</li> <li> AIXPERT on LinkedIn</li> <li> AIXPERT on X</li> <li> AIXPERT on YouTube</li> </ul>"},{"location":"about/#funding-acknowledgment","title":"Funding Acknowledgment","text":"<p>Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.</p> <p>The AIXPERT Project has also received funding from the European Union\u2019s Horizon Europe Research and Innovation Programme under Grant Agreement No. 101214389, and from the Swiss State Secretariat for Education, Research and Innovation (SERI).</p> <p>Views and opinions expressed are those of the author(s) only and do not necessarily reflect those of the European Union or the granting authorities. Neither the European Union nor the funding agencies can be held responsible for them.</p>"},{"location":"team/","title":"Team","text":"<p>The team at the Vector Institute behind the development of this project focuses on ethical AI practices, promoting fairness, accountability, and sustainability.</p> <p>For inquiries or support, contact:</p> <p></p> Shaina Raza, PhD <p>Applied ML Scientist \u2013 Responsible AI</p> <p>shaina.raza@vectorinstitute.ai</p> <p> </p> Aravind Narayanan <p>Associate Applied ML Specialist</p> <p>aravind.narayanan@vectorinstitute.ai</p> <p> </p> Ananya Raval <p>Applied ML Specialist</p> <p>ananya.raval@vectorinstitute.ai</p> <p> </p> Shweta Khushu <p>Manager, AI Engineering</p> <p>shweta.khushu@vectorinstitute.ai</p> <p> </p> Edward Chang <p>Senior Technical Program Manager</p> <p>edward.chang@vectorinstitute.ai</p> <p> </p> Sindhuja Chaduvula <p>Associate Applied ML Specialist</p> <p>sindhuja.chaduvula@vectorinstitute.ai</p> <p> </p> Ahmed Radwan <p>Applied Machine Learning Intern</p> <p>ahmed.radwan@vectorinstitute.ai</p> <p> </p> Karanpal Sekhon <p>Applied Machine Learning Intern</p> <p>karanpal.sekhon@vectorinstitute.ai</p> <p> </p>"},{"location":"team/#acknowledgements","title":"Acknowledgements","text":"<p>Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.</p> <p>Funding for the research was also partly provided through Horizon Europe project AIXpert: An agentic, multi-layer, GenAI-powered backbone to make an AI system explainable, accountable, and transparent (ID: 101214389).</p>"},{"location":"user_guide/","title":"User Guide","text":"<p>Transparent tools and standardized benchmarks for fair, explainable, and accountable generative AI. This guide introduces modules, setup, and usage patterns for fairness-aware data generation and analysis.</p>"},{"location":"user_guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started</li> <li>Core Concepts</li> <li>Contributing and Documentation</li> </ul>"},{"location":"user_guide/#getting-started","title":"Getting Started","text":"<p>Four Essential Questions</p> Question Answer What is the project about? A research framework and toolkit for generating, evaluating, and mitigating bias in multimodal AI systems. Why Fairness-Aware Synthetic Data? Controlled datasets isolate bias-inducing factors, allowing targeted experiments on fairness, explainability, and representation. Why Agentic AI? We use autonomous LLM agents (via CrewAI) to scale prompt, image, and metadata generation. Who is project for? Researchers, data scientists, and fairness practitioners studying or benchmarking bias in generative models."},{"location":"user_guide/#installation","title":"Installation","text":""},{"location":"user_guide/#from-source-recommended","title":"From source (recommended)","text":"<pre><code>git clone https://github.com/VectorInstitute/vector-aixpert.git\ncd vector-aixpert\nuv sync\n</code></pre>"},{"location":"user_guide/#optional-groups","title":"Optional groups","text":"<pre><code># Development extras\nuv sync --dev\n# Documentation build\nuv sync --no-group docs\n</code></pre>"},{"location":"user_guide/#verify-installation","title":"Verify installation","text":"<pre><code>pytest -q\nmkdocs serve\n</code></pre>"},{"location":"user_guide/#quick-starts","title":"Quick Starts","text":"<p>Minimal commands to explore modules.</p> <pre><code># 1. Set up environment\nuv sync\nsource .venv/bin/activate\n\n# 2. Run controlled image generation\ncd src/aixpert/controlled_images/\nuv run python src/main.py \\\n  --config configs/img_gen_config.yaml\n\n# 3. Generate synthetic data (text)\ncd src/aixpert/data_generation/synthetic_data_generation/nlp/\nuv run main.py \\\n  --config config.yaml \\\n  --stage all\n\n# 4. Generate synthetic data (images)\ncd src/aixpert/data_generation/synthetic_data_generation/images/\nuv run main.py all_stages \\\n  --config_file ../../config.yaml \\\n  --prompt_yaml prompt_paths.yaml \\\n  --domain hiring \\\n  --risk security_risks\n\n# 5. Compute fairness metrics\ncd src/aixpert/toxicity_fairness_analysis/\n\nuv run python scripts/download_data.py \\\n  --dataset jigsaw \\\n  --out data/jigsaw.parquet \\\n  --sample 50000\n\nuv run python scripts/llm_zero_shot_explain.py \\\n  --in data/jigsaw.parquet \\\n  --text_col comment_text \\\n  --task toxicity \\\n  --out outputs/zs_preds.parquet \\\n  --model distilgpt2 \\\n  --max_rows 1000 \\\n  --ig_rows 25 \\\n  --ig_steps 32 \\\n  --save_heatmaps \\\n  --force_float32 \\\n  --label_col target\n  --id_cols male female black white muslim jewish\n</code></pre> <p>Each module provides a focused README with configuration details and output examples.</p>"},{"location":"user_guide/#standard-usage","title":"Standard Usage","text":"<p>Typical workflow for fairness-aware data generation:</p> <ol> <li>Generate controlled data    Create matched datasets (e.g., gender, occupation, or ethnicity pairs).</li> <li>Run agentic generation pipeline    Use CrewAI agents for multimodal prompt, image, and metadata generation.</li> <li>Perform fairness analysis    Compute bias metrics such as Statistical Parity or Equal Opportunity.</li> <li>Visualize or export results    Generate structured outputs or Hugging Face datasets for benchmarking.</li> </ol>"},{"location":"user_guide/#core-concepts","title":"Core Concepts","text":""},{"location":"user_guide/#controlled-images","title":"Controlled Images","text":"<p>Generates baseline vs fairness-aware image sets for occupations or social groups. Supports configurable attributes, matched prompts, and consistent random seeds for reproducibility.</p>"},{"location":"user_guide/#synthetic-data-generation","title":"Synthetic Data Generation","text":"<p>Multi-modal data synthesis modules under:</p> <ul> <li><code>synthetic_data_generation/images</code> \u2014 image + VQA pairs</li> <li><code>synthetic_data_generation/nlp</code> \u2014 textual scenes and MCQs</li> <li><code>synthetic_data_generation/videos</code> \u2014 Veo/Gemini video generation</li> </ul> <p>Each generator is driven by LLM-designed prompts and metadata templates.</p>"},{"location":"user_guide/#agent-pipeline-crewai","title":"Agent Pipeline (CrewAI)","text":"<p>Implements single-agent orchestration to chain prompt \u2192 image \u2192 metadata generation. Enables autonomous large-scale data creation using structured JSON task definitions.</p>"},{"location":"user_guide/#fairness-explainability","title":"Fairness &amp; Explainability","text":"<p>Evaluates generated data and model outputs via:</p> <ul> <li>Statistical metrics \u2014 Statistical Parity, Equal Opportunity</li> <li>Zero-shot explainers \u2014 integrated gradients, concept attributions</li> <li>Visualization tools \u2014 disparity plots, attribution maps</li> </ul>"},{"location":"user_guide/#module-quick-start-one-liners-deep-links","title":"Module Quick Start (one-liners + deep links)","text":"<p>Each core module has its own README with commands, configurations, and sample outputs.</p> <ul> <li> <p>Controlled Images \u2014 Generate matched baseline vs fairness-aware images across professions.   \u279c <code>View Module README</code></p> </li> <li> <p>Agent Pipeline (CrewAI) \u2014 Single-agent orchestration for prompt, image, and metadata generation.   \u279c <code>View Module README</code></p> </li> <li> <p>Synthetic Data \u00b7 Images \u2014 Domain- and risk-specific image prompts and VQA pairs.   \u279c <code>View Module README</code></p> </li> <li> <p>Synthetic Data \u00b7 NLP \u2014 Scene descriptions and MCQ generation for text pipelines.   \u279c <code>View Module README</code></p> </li> </ul>"},{"location":"user_guide/#contributing-and-documentation","title":"Contributing and Documentation","text":"<p>See CONTRIBUTING.md for:</p> <ul> <li>Coding standards and style guide (PEP8 + Google docstrings)</li> <li>Pre-commit setup (<code>ruff</code>, <code>mypy</code>, <code>typos</code>, <code>nbQA</code>)</li> <li>Branching and PR workflow</li> <li>Test coverage requirements</li> </ul>"},{"location":"user_guide/#docs-build","title":"Docs build","text":"<pre><code>uv sync --no-group docs\nmkdocs serve\n</code></pre> <p>The site will be live at http://127.0.0.1:8000.</p>"},{"location":"user_guide/#testing-and-standards","title":"Testing and Standards","text":"<pre><code>pytest -v\npre-commit run --all-files\n</code></pre> <p>Continuous integration runs these via GitHub Actions (<code>code_checks.yml</code>, <code>unit_tests.yml</code>, <code>integration_tests.yml</code>).</p>"}]}